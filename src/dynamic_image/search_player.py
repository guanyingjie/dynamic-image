import requests
from bs4 import BeautifulSoup
from urllib.parse import quote
import re
import time
import json

# ================= é…ç½®åŒº =================
# ä½ çš„ Cloudflare Worker ä»£ç†åœ°å€
WORKER_URL = "https://search.yingjie.icu"


# =========================================

class KyurekiUltimateSpider:
    def __init__(self):
        # ä¼ªè£…å¤´ï¼šæ¨¡æ‹Ÿ Googlebotï¼Œè¿™æ˜¯ç©¿é€ 403 æœ€æœ‰æ•ˆçš„ä¼ªè£…ä¹‹ä¸€
        self.headers = {
            "User-Agent": "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
        }

    def _request_via_worker(self, target_url):
        """é€šè¿‡ Worker ä»£ç†å‘é€è¯·æ±‚"""
        proxy_url = f"{WORKER_URL}?url={quote(target_url)}"
        try:
            return requests.get(proxy_url, headers=self.headers, timeout=25)
        except Exception as e:
            print(f"[!] Worker è¯·æ±‚å¼‚å¸¸: {e}")
            return None

    def search_player_id(self, name):
        """æ­¥éª¤1: é€šè¿‡ Yahoo Japan æœç´¢çƒå‘˜ ID"""
        print(f"[*] æ­£åœ¨å…¨ç½‘æœç´¢: {name} ...")
        # Yahoo JP å¯¹çƒå†ç½‘çš„æ”¶å½•éå¸¸å…¨
        query = f"site:kyureki.com {name}"
        search_url = f"https://search.yahoo.co.jp/search?p={quote(query)}"

        # æœç´¢ä¹Ÿèµ°ä»£ç†ï¼Œé˜²æ­¢ Yahoo å°é”æœ¬åœ° IP
        resp = self._request_via_worker(search_url)

        if not resp or resp.status_code != 200:
            print("[-] æœç´¢è¯·æ±‚å¤±è´¥ (å¯èƒ½ Worker æš‚æ—¶ä¸å¯ç”¨)ã€‚")
            return None

        soup = BeautifulSoup(resp.text, 'html.parser')
        # æŸ¥æ‰¾åŒ…å« /player/ çš„é“¾æ¥
        for a in soup.find_all('a', href=True):
            if "kyureki.com/player/" in a['href']:
                # æ¸…æ´— URL
                clean_url = a['href'].split('?')[0]
                if clean_url.startswith("http:"):
                    clean_url = clean_url.replace("http:", "https:")
                print(f"[+] æ‰¾åˆ°çƒå‘˜ä¸»é¡µ: {clean_url}")
                return clean_url

        print("[-] æœªæ‰¾åˆ°è¯¥çƒå‘˜çš„çƒå†ç½‘é¡µé¢ã€‚")
        return None

    def get_player_data(self, url):
        """æ­¥éª¤2: æ™ºèƒ½è·å–æ•°æ® (ä¼˜å…ˆæœ€æ–°ï¼Œå¤±è´¥åˆ™é€šè¿‡æ—¶å…‰æœº)"""

        # --- ç­–ç•¥ A: å°è¯•è·å–æœ€æ–°æ•°æ® ---
        print("[*] ç­–ç•¥ A: å°è¯•é€šè¿‡ä»£ç†è·å–æœ€æ–°æ•°æ®...")
        resp = self._request_via_worker(url)

        # æ£€æŸ¥æ˜¯å¦æˆåŠŸ (æœ‰äº› 403 ä¼šè¿”å› 200 ä½†å†…å®¹æ˜¯ Access Denied)
        if resp and resp.status_code == 200 and "Forbidden" not in resp.text and "Access Denied" not in resp.text:
            print("[+] æˆåŠŸè¿æ¥å®æ—¶ç½‘ç«™ï¼")
            self.parse_html(resp.text, source="å®æ—¶æ•°æ®")
            return

        # --- ç­–ç•¥ B: å¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ—¶å…‰æœº ---
        print(f"[-] ç­–ç•¥ A é­é‡é˜²ç«å¢™æ‹¦æˆª (çŠ¶æ€ç : {resp.status_code if resp else 'Error'})")
        print("[*] ç­–ç•¥ B: å¯åŠ¨ Archive.org æ—¶å…‰æœºæ•‘æ´...")
        self.get_from_archive(url)

    def get_from_archive(self, original_url):
        """ä» Archive.org è·å–æœ€è¿‘çš„å¿«ç…§"""
        # æŸ¥è¯¢æœ€è¿‘çš„å¿«ç…§ API
        api_url = f"https://archive.org/wayback/available?url={original_url}"

        try:
            # è¿™ä¸ªè¯·æ±‚å¯ä»¥ç›´æ¥æœ¬åœ°å‘ï¼ŒArchive.org ä¸æ€ä¹ˆå° IP
            api_resp = requests.get(api_url, timeout=15)
            data = api_resp.json()

            if not data.get('archived_snapshots'):
                print("[-] é—æ†¾ï¼šArchive.org å°šæœªæ”¶å½•è¯¥é¡µé¢ã€‚")
                return

            snapshot_url = data['archived_snapshots']['closest']['url']
            print(f"[+] æ‰¾åˆ°å†å²å¿«ç…§: {snapshot_url}")

            # ä¸‹è½½å¿«ç…§å†…å®¹
            content_resp = requests.get(snapshot_url, headers=self.headers, timeout=30)
            if content_resp.status_code == 200:
                self.parse_html(content_resp.text, source="å†å²å¿«ç…§")
            else:
                print(f"[!] å¿«ç…§ä¸‹è½½å¤±è´¥: {content_resp.status_code}")

        except Exception as e:
            print(f"[!] Archive æ­¥éª¤å‡ºé”™: {e}")

    def parse_html(self, html, source="æœªçŸ¥"):
        """æ ¸å¿ƒè§£æå™¨: å…¼å®¹æ–°æ—§ä¸¤ç§ HTML ç»“æ„"""
        soup = BeautifulSoup(html, 'html.parser')

        # 1. æå–åå­—
        name_tag = soup.find('h1')
        if name_tag:
            name = name_tag.get_text(strip=True)
        else:
            name = soup.title.string if soup.title else "æœªçŸ¥çƒå‘˜"

        print("\n" + "â”" * 50)
        print(f"âš¾  çƒå‘˜æ¡£æ¡ˆ: {name}")
        print(f"   (æ•°æ®æ¥æº: {source})")
        print("â”" * 50)

        # 2. éå†è¡¨æ ¼è¡Œ
        rows = soup.find_all('tr')
        resume_data = None  # æš‚å­˜å±¥å†

        for row in rows:
            key = None
            val_cell = None

            # --- é€‚é…é€»è¾‘ ---
            # æƒ…å†µ 1: æ ‡å‡†ç»“æ„ <th>Key</th> <td>Value</td>
            if row.find('th'):
                key = row.find('th').get_text(strip=True)
                val_cell = row.find('td')

            # æƒ…å†µ 2: Archive/æ—§ç‰ˆç»“æ„ <td><b>Key</b></td> <td>Value</td>
            else:
                cells = row.find_all('td')
                if len(cells) >= 2 and cells[0].find('b'):
                    key = cells[0].find('b').get_text(strip=True)
                    val_cell = cells[1]

            # --- æ•°æ®å¤„ç† ---
            if key and val_cell:
                # æ’é™¤ Archive æ³¨å…¥çš„å¹²æ‰°è¡Œ
                if "Capture" in key or "Wayback" in key:
                    continue

                # A. å¤„ç† "å…¨å›½å¤§ä¼š" æˆ– "æˆ˜ç»©" (å¤šè¡Œåˆ—è¡¨)
                if "å…¨å›½å¤§ä¼š" in key or "æˆ¦ç¸¾" in key:
                    print(f"ã€ {key} ã€‘")
                    # get_text(separator="\n") è‡ªåŠ¨æŠŠ <br> å˜æˆæ¢è¡Œ
                    lines = val_cell.get_text(separator="\n").split('\n')
                    for line in lines:
                        if line.strip():
                            print(f"  â€¢ {line.strip()}")

                # B. å¤„ç† "å±¥å†" (æš‚å­˜ï¼Œæœ€åç”»å›¾)
                elif "çµŒæ­´" in key:
                    resume_data = val_cell.get_text(strip=True)

                # C. æ™®é€šå­—æ®µ
                else:
                    val = val_cell.get_text(strip=True)
                    # åªæ‰“å°éç©ºçš„æœ‰æ•ˆä¿¡æ¯
                    if val:
                        # å…¨è§’ç©ºæ ¼å¯¹é½
                        print(f"{key.ljust(6, chr(12288))}: {val}")

        print("-" * 50)

        # 3. ç»˜åˆ¶å±¥å†è·¯å¾„å›¾
        if resume_data:
            print("ã€ ğŸ“… èŒä¸šå±¥å†è·¯å¾„ ã€‘")
            # æ‹†åˆ†è·¯å¾„ (å…¼å®¹ç®­å¤´ > ï¼ å’Œç©ºæ ¼)
            parts = re.split(r'[>ï¼\sã€€]+', resume_data)
            parts = [p for p in parts if p]

            if parts:
                print("START")
                for i, part in enumerate(parts):
                    prefix = "  â–¼" if i > 0 else "  â”‚"
                    # è¿™é‡Œé€»è¾‘å¾®è°ƒï¼Œç¡®ä¿ç®­å¤´åœ¨æ¯è¡Œä¹‹é—´
                    if i > 0:
                        print(f"  â–¼")
                    print(f"  â”‚  {part}")
                print("END")
            else:
                print(resume_data)

        print("â”" * 50 + "\n")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    spider = KyurekiUltimateSpider()

    # åœ¨è¿™é‡Œè¾“å…¥ä½ æƒ³æœç´¢çš„çƒå‘˜åå­—
    # å»ºè®®ä½¿ç”¨æ—¥æ–‡æ±‰å­—ä»¥æé«˜å‡†ç¡®ç‡
    target_name = "å®—å±±å¡"

    # è¿è¡Œ
    player_url = spider.search_player_id(target_name)

    if player_url:
        spider.get_player_data(player_url)